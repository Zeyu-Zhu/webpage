<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zeyu Zhu</title>

    <meta name="author" content="Zeyu Zhu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zeyu Zhu
                </p>
                <p>I am a first-year PhD student at <a href="https://sites.google.com/view/showlab">Showlab</a> supervised by <a href="https://sites.google.com/view/showlab">Prof. Mike Shou</a> in National University of Singapore. 
                </p>
                <p>And I am very fortunate to be advised by <a href="https://gr.xjtu.edu.cn/en/web/caoxiangyong/home">Prof. Xiangyong Cao</a> and <a href="https://gr.xjtu.edu.cn/en/web/dymeng/1">Prof. Deyu Meng</a> throughout my undergraduate years.
                </p>
                <p style="text-align:center">
                  <a href="mailto:zeyuzhu2077@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=X3CisOwAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Zeyu-Zhu">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/ZHUZEYU.JPG"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/ZHUZEYU.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision and generative AI: Diffusion, Agent for Video Generation.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="work_assets/paper2video.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://github.com/showlab/Paper2Video" id="MCG_journal">
                  <span class="papertitle">Paper2Video: Automatic Video Generation from Scientific Papers</span>
                </a>
                <br>
                <strong>Zeyu Zhu*</strong>, Kevin Qinghong Lin* , Mike Zheng Shou  
                <br>
                <em> Scaling Environments for Agents Workshop at NeurIPS</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2510.05096">arxiv</a> /
                <a href="https://showlab.github.io/Paper2Video/">page</a> /
                <a href="https://github.com/showlab/Paper2Video">github</a> /
                <a href="https://huggingface.co/datasets/ZaynZhu/Paper2Video">datasets</a> / 
                <a href="https://huggingface.co/papers/2510.05096">paper of the day on huggingface</a> 
                <p>We introduce Paper2Video, a benchmark and multi-agent framework (PaperTalker) that automates academic presentation video generation from papers, integrating slides, subtitles, speech, and talking-heads with evaluation metrics to ensure faithfulness and informativeness.</p>
              </td>
            </tr>
          <tr>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="work_assets/multi-human.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://zeyu-zhu.github.io/multi_human_interactive_talking/" id="MCG_journal">
                  <span class="papertitle">Multi-human Interactive Talking Datasets</span>
                </a>
                <br>
                <strong>Zeyu Zhu</strong>, Weijia Wu, Mike Zheng Shou  
                <br>
                <em>Arxiv</em>, 2025
                <br>
                <a href="https://arxiv.org/pdf/2508.03050">arxiv</a> /
                <a href="https://zeyu-zhu.github.io/multi_human_interactive_talking/">page</a> /
                <a href="https://github.com/showlab/Multi-human-Talking-Video-Dataset">github</a> /
                <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/FJ8MRB">datasets</a>
                <p>We introduces a multi-human talking dataset (MIT) and baseline model (CovOG) for generating realistic multi-human talking videos, addressing the limitations of single-speaker approaches.</p>
              </td>
            </tr>
          <tr>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/MovieAgent.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://weijiawu.github.io/MovieAgent/" id="MCG_journal">
                  <span class="papertitle">MovieAgent: Automated Movie Generation via Multi-Agent CoT Planning</span>
                </a>
                <br>
                Weijia Wu, <strong>Zeyu Zhu</strong>, Mike Zheng Shou  
                <br>
                <em>Arxiv</em>, 2025
                <br>
                <a href="https://weijiawu.github.io/MovieAgent/">page</a> /
                <a href="https://arxiv.org/pdf/2503.07314">arxiv</a> 
                <p>We propose MovieAgent, a multi-agent Chain-of-Thought framework that automates long-form movie generation from scripts, coordinating scene planning, cinematography, and character consistency to achieve coherent, faithful, and fully automated film production.</p>
              </td>
            </tr>
            
          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/MovieBen.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://weijiawu.github.io/MovieBench/" id="MCG_journal">
                  <span class="papertitle">MovieBench: A Hierarchical Movie Level Dataset for Long Video Generation</span>
                </a>
                <br>
                Weijia Wu, Mingyu Liu, <strong>Zeyu Zhu</strong>, Xi Xia, Haoen Feng, Wen Wang, Kevin Qinghong Lin, Chunhua Shen, Mike Zheng Shou  
                <br>
                <em>The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2025
                <br>
                <a href="https://weijiawu.github.io/MovieBench/">page</a> /
                <a href="https://arxiv.org/abs/2411.15262">arxiv</a> 
                <p>We propose a hierarchical movie level dataset for long video generation.</p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/low-rank diffusion model.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253524001039" id="MCG_journal">
                  <span class="papertitle">Unsupervised Hyperspectral Pansharpening via Low-rank Diffusion Model</span>
                </a>
                <br>
                Xiangyu Rui, Xiangyong Cao, Li Pang, <strong>Zeyu Zhu</strong>, Zongsheng Lyu, DeyuMeng
                <br>
                <em>Information Fusion(IF=18.6)</em>, 2024
                <br>
                <a href="https://github.com/xyrui/PLRDiff?tab=readme-ov-file">github</a> /
                <a href="https://arxiv.org/pdf/2305.10925">arxiv</a> 
                <p>We propose a low-rank diffusion model for hyperspectral pansharpening by leveraging the power of the pre-trained deep diffusion model and better generalization ability of Bayesian methods.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/PGCU.png" alt="PontTuset" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zhu_Probability-Based_Global_Cross-Modal_Upsampling_for_Pansharpening_CVPR_2023_paper.html" id="MCG_journal">
                  <span class="papertitle">Probability-based Global Cross-modal Upsampling for Pansharpening</span>
                </a>
                <br>
                <strong>Zeyu Zhu</strong>, Xiangyong Cao, Man Zhou, Junhao Huang, Deyu Meng
                <br>
                <em>The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2023
                <br>
                <a href="https://github.com/Zeyu-Zhu/PGCU">github</a> /
                <a href="http://arxiv.org/abs/2303.13659">arxiv</a> 
                <p>We propose a novel probability-based global cross-modal upsampling (PGCU) method for pan-sharpening to to exploit global information of the LRMS image as well as the cross-modal information of the PAN image which can be plug-and-played into existing models.</p>
              </td>
            </tr>

          </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Education</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/NUS.png" alt="PontTuset" width=110" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <span class="papertitle">National Unverisity of Singapore(NUS)</span>, Singapore
                <p></p>
                Ph.D. in Electrical and Computer Engineering
                <p></p>
                Aug. 2024 -
                <p></p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/XJTU.jpg" alt="PontTuset" width="120" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <span class="papertitle">Xi'an Jiaotong University(XJTU)</span>, China
                <p></p>
                B.E. in Artificial Intelligence
                <p></p>
               	Sep. 2020 - Jul. 2024
                <p></p>
              </td>
            </tr>
            
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ShanghaiAILab.png" alt="PontTuset" width="120" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://idc-sh.github.io/" id="MCG_journal">
                  <span class="papertitle">Shanghai AI Lab</span> </a>, China
                <br>
                <strong>Research Intern</strong>
                <br>
                June. 2023 - June. 2024
                <br>
                Focus: Editable 3D Object Generation
                <p></p>
              </td>
            </tr>
            
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Honors & Awards</h2>
                 <p>
                &bull; <strong>School Scholarship</strong> -- AY20/21, AY21/22
                <br>
                &bull; <strong>Summer Workshop at School of Computing of NUS</strong>, First Prize -- AY21/22
                <br>
                &bull; <strong>Supermarket Shopping Service Robots in China Robot Competition</strong>, First Prize -- AY21/22
                <br>
                &bull; <strong>China Telecom First Class Scholarship</strong> -- AY22/23
                </p>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

        <div style="text-align: center;">
           <a href="https://clustrmaps.com/site/1bzi8"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=vJ2iQZHqueQfkOgkoZ_uLaKlYHLaD8Dyt-Y3Z0hk8E8&cl=ffffff" /></a>
        </div> 
          
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last updated in Dec 2024.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
